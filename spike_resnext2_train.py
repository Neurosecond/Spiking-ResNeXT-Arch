import torch
import torch.nn as nn
import torch.nn.functional as F
import snntorch as snn
from sklearn.metrics import confusion_matrix
import numpy as np
import pandas as pd
import MetaTrader5 as mt5
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
from datetime import datetime, timedelta
import warnings
import os

warnings.filterwarnings('ignore')


# =============================================================================
# 1. –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–ë–û–¢–´ –° META TRADER 5
# =============================================================================

def initialize_mt5():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ MetaTrader5"""
    if not mt5.initialize():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å MT5")
        return False
    print("‚úÖ MT5 —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    return True


def download_mt5_data(symbol="EURUSDrfd", bars_count=20000):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ MetaTrader5 –¥–ª—è EURUSD
    """
    timeframes = {
        'M5': mt5.TIMEFRAME_M5,
        'M15': mt5.TIMEFRAME_M15,
        'H1': mt5.TIMEFRAME_H1,
        'H4': mt5.TIMEFRAME_H4
    }

    data_dict = {}

    for tf_name, tf_enum in timeframes.items():
        try:
            print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} {tf_name}...")

            rates = mt5.copy_rates_from_pos(symbol, tf_enum, 0, bars_count)

            if rates is None:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {tf_name}")
                continue

            df = pd.DataFrame(rates)
            df['datetime'] = pd.to_datetime(df['time'], unit='s')
            df.set_index('datetime', inplace=True)
            df = df[['open', 'high', 'low', 'close', 'tick_volume']]
            data_dict[tf_name] = df

            print(f"‚úÖ {tf_name}: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –±–∞—Ä–æ–≤")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {tf_name}: {e}")

    return data_dict


def create_mock_eurusd_data():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è EURUSD"""
    print("üîÑ –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ EURUSD...")

    base_dates = pd.date_range(start='2023-01-01', end='2024-01-01', freq='5min')[:5000]

    mock_data = {}

    timeframe_configs = [
        ('M5', 1),
        ('M15', 3),
        ('H1', 12),
        ('H4', 48)
    ]

    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è M5 —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º EURUSD
    np.random.seed(42)
    base_prices = []
    price = 1.1000  # –ù–∞—á–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ EURUSD

    for i in range(len(base_dates)):
        # –ë–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è EURUSD
        trend = 0.00001  # –°–ª–∞–±—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥
        volatility = 0.0008  # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å EURUSD
        change = np.random.normal(trend, volatility)
        price = max(0.9, min(1.3, price * (1 + change)))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        base_prices.append(price)

    for tf_name, multiplier in timeframe_configs:
        data = []

        for i in range(0, len(base_prices), multiplier):
            if i >= len(base_prices):
                break

            base_idx = i
            open_price = base_prices[base_idx]

            period_high = max(base_prices[base_idx:min(base_idx + multiplier, len(base_prices))])
            period_low = min(base_prices[base_idx:min(base_idx + multiplier, len(base_prices))])

            close_idx = min(base_idx + multiplier - 1, len(base_prices) - 1)
            close_price = base_prices[close_idx]

            volume = np.random.randint(100 * multiplier, 1000 * multiplier)

            data.append({
                'open': open_price,
                'high': period_high,
                'low': period_low,
                'close': close_price,
                'tick_volume': volume
            })

        df = pd.DataFrame(data)

        if tf_name == 'M5':
            df.index = base_dates[:len(df)]
        else:
            df.index = base_dates[::multiplier][:len(df)]

        mock_data[tf_name] = df
        print(f"‚úÖ {tf_name}: —Å–æ–∑–¥–∞–Ω–æ {len(df)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –±–∞—Ä–æ–≤")

    return mock_data


# =============================================================================
# 2. –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ö–û–î–ò–†–û–í–©–ò–ö –î–ê–ù–ù–´–• –í –°–ü–ê–ô–ö–ò
# =============================================================================

class FinancialSpikeEncoder:
    """–ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Å–ø–∞–π–∫–∏"""

    def __init__(self, num_time_steps=50, threshold_std=1.0):
        self.num_time_steps = num_time_steps
        self.threshold_std = threshold_std

    def price_change_encoding(self, price_data):
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ü–µ–Ω—ã –≤ —Å–ø–∞–π–∫–∏"""
        # price_data shape: (batch_size, seq_len, num_features)
        batch_size, seq_len, num_features = price_data.shape

        # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
        price_changes = np.diff(price_data, axis=1)  # (batch_size, seq_len-1, num_features)

        # –°–æ–∑–¥–∞–µ–º —Å–ø–∞–π–∫–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä
        spike_tensor = np.zeros((batch_size, 3, seq_len, self.num_time_steps))

        for batch_idx in range(batch_size):
            for feature_idx in range(num_features):
                feature_changes = price_changes[batch_idx, :, feature_idx]

                if len(feature_changes) == 0:
                    continue

                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
                mean_change = np.mean(feature_changes)
                std_change = np.std(feature_changes)

                if std_change < 1e-8:
                    normalized_changes = np.zeros_like(feature_changes)
                else:
                    normalized_changes = (feature_changes - mean_change) / std_change

                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∞–π–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Ä–æ–≥–æ–≤
                for step_idx, change in enumerate(normalized_changes):
                    if step_idx >= seq_len - 1:  # –ó–∞—â–∏—Ç–∞ –æ—Ç –≤—ã—Ö–æ–¥–∞ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã
                        continue

                    if change > self.threshold_std:
                        # –°–ø–∞–π–∫ –≤–≤–µ—Ä—Ö
                        spike_indices = np.linspace(0, self.num_time_steps - 1, 3, dtype=int)
                        spike_tensor[batch_idx, 0, step_idx, spike_indices] = 1
                    elif change < -self.threshold_std:
                        # –°–ø–∞–π–∫ –≤–Ω–∏–∑
                        spike_indices = np.linspace(0, self.num_time_steps - 1, 3, dtype=int)
                        spike_tensor[batch_idx, 1, step_idx, spike_indices] = 1
                    else:
                        # –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                        spike_indices = np.random.choice(self.num_time_steps, 1)
                        spike_tensor[batch_idx, 2, step_idx, spike_indices] = 1

        return torch.FloatTensor(spike_tensor)


# =============================================================================
# 3. –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –°–ü–ê–ô–ö–û–í–û–ô RESNEXT (–ë–ï–ó –°–û–•–†–ê–ù–ï–ù–ò–Ø –°–û–°–¢–û–Ø–ù–ò–Ø)
# =============================================================================

class MultiTimeframeSpikingResNeXt(nn.Module):
    """–°–ø–∞–π–∫–æ–≤–∞—è ResNeXt –¥–ª—è –º—É–ª—å—Ç–∏—Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ EURUSD"""

    def __init__(self, num_timeframes=4, num_classes=3, num_steps=50,
                 cardinality=16, beta=0.9):
        super().__init__()

        self.num_steps = num_steps
        self.num_timeframes = num_timeframes

        # –û–î–ù–ê –æ–±—â–∞—è –≤—Ö–æ–¥–Ω–∞—è —Å–≤–µ—Ä—Ç–∫–∞ –¥–ª—è –≤—Å–µ—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.lif1 = snn.Leaky(beta=beta, learn_beta=True)
        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # ResNeXt –±–ª–æ–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ features
        self.resnext_blocks = nn.Sequential(
            SparseSpikingResNeXtBlock(64, 128, stride=2, cardinality=cardinality),
            SparseSpikingResNeXtBlock(128, 256, stride=2, cardinality=cardinality),
            SparseSpikingResNeXtBlock(256, 512, stride=1, cardinality=cardinality),
        )

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.classifier = nn.Sequential(
            nn.Linear(512 * num_timeframes, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        # x shape: (batch, num_timeframes, 3, height, width, num_steps)
        batch_size = x.shape[0]
        num_tf = x.shape[1]
        height = x.shape[3]
        width = x.shape[4]
        num_steps = x.shape[5]

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–º–±—Ä–∞–Ω–Ω—ã—Ö –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–≤ (–í–†–ï–ú–ï–ù–ù–´–ï –¥–ª—è —ç—Ç–æ–≥–æ –≤—ã–∑–æ–≤–∞)
        mem1 = self.lif1.init_leaky()

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥
        timeframe_features = []

        for tf_idx in range(num_tf):
            tf_step_features = []

            for step in range(num_steps):
                x_tf_step = x[:, tf_idx, :, :, :, step]  # (batch, 3, height, width)

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—Ö–æ–¥–Ω—É—é —Å–≤–µ—Ä—Ç–∫—É
                x_conv = self.conv1(x_tf_step)
                x_conv = self.bn1(x_conv)
                x_conv, mem1 = self.lif1(x_conv, mem1)  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å
                x_conv = self.pool1(x_conv)

                # –ü—Ä–∏–º–µ–Ω—è–µ–º ResNeXt –±–ª–æ–∫–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é
                x_features = self._forward_resnext_blocks(x_conv)

                # Global average pooling –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è feature vector
                x_pooled = F.adaptive_avg_pool2d(x_features, (1, 1))
                x_pooled = x_pooled.view(batch_size, -1)  # (batch, 512)

                tf_step_features.append(x_pooled)

            # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —à–∞–≥–∞–º –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
            tf_all_steps = torch.stack(tf_step_features, dim=0)  # (num_steps, batch, 512)
            tf_avg = tf_all_steps.mean(dim=0)  # (batch, 512)
            timeframe_features.append(tf_avg)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º features –æ—Ç –≤—Å–µ—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        combined_features = torch.cat(timeframe_features, dim=1)  # (batch, num_tf * 512)

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        output = self.classifier(combined_features)

        return output

    def _forward_resnext_blocks(self, x):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ ResNeXt –±–ª–æ–∫–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é"""
        for block in self.resnext_blocks:
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ —Å–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ–º–±—Ä–∞–Ω–Ω—ã–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—ã
            if isinstance(block, SparseSpikingResNeXtBlock):
                x = block.forward_with_temp_mem(x)
            else:
                x = block(x)
        return x

    def reset_mem(self):
        """–¢–µ–ø–µ—Ä—å —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –Ω–µ –Ω—É–∂–Ω–∞, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        pass


class SparseSpikingResNeXtBlock(nn.Module):
    """–°–ø–∞–π–∫–æ–≤—ã–π –±–ª–æ–∫ ResNeXt —Å –≥—Ä—É–ø–ø–æ–≤—ã–º–∏ —Å–≤–µ—Ä—Ç–∫–∞–º–∏"""

    def __init__(self, in_channels, out_channels, stride=1, cardinality=32,
                 width_factor=4, beta=0.9):
        super().__init__()

        self.beta = beta
        intermediate_channels = cardinality * width_factor

        self.conv1 = nn.Conv2d(in_channels, intermediate_channels,
                               kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(intermediate_channels)

        self.conv2 = nn.Conv2d(intermediate_channels, intermediate_channels,
                               kernel_size=3, stride=stride, padding=1,
                               groups=cardinality, bias=False)
        self.bn2 = nn.BatchNorm2d(intermediate_channels)

        self.conv3 = nn.Conv2d(intermediate_channels, out_channels,
                               kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels)

        self.lif1 = snn.Leaky(beta=beta, learn_beta=True)
        self.lif2 = snn.Leaky(beta=beta, learn_beta=True)
        self.lif3 = snn.Leaky(beta=beta, learn_beta=True)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                          stride=stride, bias=False),
                nn.BatchNorm2d(out_channels),
            )
            self.lif_shortcut = snn.Leaky(beta=beta, learn_beta=True)
            self.use_shortcut_lif = True
        else:
            self.use_shortcut_lif = False

        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

    def forward(self, x):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return self.forward_with_temp_mem(x)

    def forward_with_temp_mem(self, x):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ–º–±—Ä–∞–Ω–Ω—ã–º–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞–º–∏"""
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏
        mem1 = self.lif1.init_leaky()
        mem2 = self.lif2.init_leaky()
        mem3 = self.lif3.init_leaky()

        if self.use_shortcut_lif:
            mem_shortcut = self.lif_shortcut.init_leaky()

        # –û—Å–Ω–æ–≤–Ω–æ–π –ø—É—Ç—å
        out = self.conv1(x)
        out = self.bn1(out)
        out, mem1 = self.lif1(out, mem1)

        out = self.conv2(out)
        out = self.bn2(out)
        out, mem2 = self.lif2(out, mem2)

        out = self.conv3(out)
        out = self.bn3(out)

        # Shortcut –ø—É—Ç—å
        residual = self.shortcut(x)
        if self.use_shortcut_lif:
            residual, mem_shortcut = self.lif_shortcut(residual, mem_shortcut)

        # –°–ª–æ–∂–µ–Ω–∏–µ –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∞–π–∫
        out += residual
        out, mem3 = self.lif3(out, mem3)

        return out

    def reset_mem(self):
        """–¢–µ–ø–µ—Ä—å –Ω–µ –Ω—É–∂–µ–Ω"""
        pass


# =============================================================================
# 4. –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –î–ê–¢–ê–°–ï–¢
# =============================================================================

class EURUSDDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –º—É–ª—å—Ç–∏—Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö EURUSD"""

    def __init__(self, data_dict, lookback_window=100, num_steps=50, target_timeframe='M5'):
        self.data_dict = data_dict
        self.lookback_window = lookback_window
        self.num_steps = num_steps
        self.target_timeframe = target_timeframe
        self.encoder = FinancialSpikeEncoder(num_time_steps=num_steps)

        self.expected_timeframes = ['M5', 'M15', 'H1', 'H4']

        self._validate_and_preprocess_data()
        self._create_timeline()

    def _validate_and_preprocess_data(self):
        self.processed_data = {}
        self.available_timeframes = []

        print("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã...")

        for tf_name in self.expected_timeframes:
            if tf_name in self.data_dict and self.data_dict[tf_name] is not None:
                df = self.data_dict[tf_name]

                if len(df) > self.lookback_window:
                    self.available_timeframes.append(tf_name)
                    print(f"   ‚úÖ {tf_name}: {len(df)} –±–∞—Ä–æ–≤")

                    features = self._preprocess_dataframe(df)
                    self.processed_data[tf_name] = features
                else:
                    print(f"   ‚ö†Ô∏è {tf_name}: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(df)} –±–∞—Ä–æ–≤)")
            else:
                print(f"   ‚ùå {tf_name}: –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")

        if not self.available_timeframes:
            raise ValueError("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö")

        if self.target_timeframe not in self.processed_data:
            if 'M5' in self.processed_data:
                self.target_timeframe = 'M5'
            else:
                self.target_timeframe = self.available_timeframes[0]
            print(f"‚ö†Ô∏è –¶–µ–ª–µ–≤–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ {self.target_timeframe}")

        print(f"üéØ –¶–µ–ª–µ–≤–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º: {self.target_timeframe}")
        print(f"üìä –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã: {self.available_timeframes}")

    def _preprocess_dataframe(self, df):
        required_columns = ['open', 'high', 'low', 'close', 'tick_volume']

        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            available_cols = list(df.columns)
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}. –î–æ—Å—Ç—É–ø–Ω—ã: {available_cols}")

        features = df[required_columns].values

        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        price_columns = ['open', 'high', 'low', 'close']
        price_indices = [required_columns.index(col) for col in price_columns]

        for idx in price_indices:
            features[:, idx] = np.log(features[:, idx])

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–º–∞
        volume_idx = required_columns.index('tick_volume')
        features[:, volume_idx] = np.log1p(features[:, volume_idx])

        return features

    def _create_timeline(self):
        target_data = self.processed_data[self.target_timeframe]

        self.valid_indices = list(range(
            self.lookback_window,
            len(target_data) - 1
        ))

        print(f"üìà –î–æ—Å—Ç—É–ø–Ω–æ samples: {len(self.valid_indices)}")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        real_idx = self.valid_indices[idx]
        multi_tf_data = []

        timeframe_ratios = {
            'M5': 1,
            'M15': 3,
            'H1': 12,
            'H4': 48
        }

        for tf_name in self.available_timeframes:
            data = self.processed_data[tf_name]
            ratio = timeframe_ratios.get(tf_name, 1)

            window_start = max(0, (real_idx - self.lookback_window) // ratio)
            window_end = real_idx // ratio

            if window_end >= len(data):
                window_end = len(data) - 1
                window_start = max(0, window_end - (self.lookback_window // ratio))

            window_data = data[window_start:window_end]

            if len(window_data) < self.lookback_window:
                padding_needed = self.lookback_window - len(window_data)
                padding = np.tile(window_data[0:1], (padding_needed, 1))
                window_data = np.vstack([padding, window_data])

            # –ö–æ–¥–∏—Ä—É–µ–º –≤ —Å–ø–∞–π–∫–∏
            window_data_reshaped = window_data.reshape(1, self.lookback_window, 5)
            spikes = self.encoder.price_change_encoding(window_data_reshaped)

            # spikes shape: (1, 3, lookback_window, num_steps)
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫ (3, lookback_window, num_steps)
            spikes = spikes.squeeze(0)

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–æ–±–∞–≤–ª—è–µ–º –∏–∑–º–µ—Ä–µ–Ω–∏–µ –≤—ã—Å–æ—Ç—ã (–¥–µ–ª–∞–µ–º lookback_window –∫–∞–∫ height)
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º (3, lookback_window, num_steps) -> (3, lookback_window, 1, num_steps)
            spikes = spikes.unsqueeze(2)  # –¥–æ–±–∞–≤–ª—è–µ–º width=1

            multi_tf_data.append(spikes)

        # –°–æ–∑–¥–∞–µ–º –æ–±—â–∏–π —Ç–µ–Ω–∑–æ—Ä: (num_timeframes, 3, height, width, num_steps)
        # –≥–¥–µ height = lookback_window, width = 1
        multi_tf_tensor = torch.stack(multi_tf_data)

        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        target_data = self.processed_data[self.target_timeframe]
        future_price = target_data[real_idx + 1, 3]
        current_price = target_data[real_idx, 3]

        price_ratio = np.exp(future_price - current_price)

        # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ (0.1-0.5%)
        threshold = 0.00007 if self.target_timeframe == 'M5' else 0.00005

        if price_ratio > 1 + threshold:
            target = 2  # BUY
        elif price_ratio < 1 - threshold:
            target = 0  # SELL
        else:
            target = 1  # HOLD

        return multi_tf_tensor, torch.tensor(target, dtype=torch.long)

# =============================================================================
# 5+ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
# =============================================================================
def analyze_dataset(dataloader):
    targets_count = {0: 0, 1: 0, 2: 0}  # SELL, HOLD, BUY

    for _, targets in dataloader:
        # targets - —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é [batch_size]
        for target in targets:
            cls = target.item()  # —Ç–µ–ø–µ—Ä—å —ç—Ç–æ —Å–∫–∞–ª—è—Ä
            targets_count[cls] = targets_count.get(cls, 0) + 1

    total_samples = sum(targets_count.values())
    print("=== –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï TARGETS –í –î–ê–¢–ê–°–ï–¢–ï ===")
    for cls, count in targets_count.items():
        class_name = ["SELL", "HOLD", "BUY"][cls]
        percentage = count / total_samples * 100
        print(f"{class_name}: {count} samples ({percentage:.1f}%)")

    return targets_count


def analyze_model_predictions(model, dataloader, device):
    model.eval()
    model.to(device)  # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ

    predictions_count = {0: 0, 1: 0, 2: 0}
    correct_predictions = {0: 0, 1: 0, 2: 0}
    total_predictions = 0

    with torch.no_grad():
        for data, targets in dataloader:
            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å
            data = data.to(device)
            targets = targets.to(device)

            outputs = model(data)
            predictions = outputs.argmax(dim=1)  # [batch_size]

            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ CPU –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞
            predictions_cpu = predictions.cpu()
            targets_cpu = targets.cpu()

            for pred, target in zip(predictions_cpu, targets_cpu):
                pred_cls = pred.item()
                target_cls = target.item()

                predictions_count[pred_cls] += 1
                if pred_cls == target_cls:
                    correct_predictions[pred_cls] += 1
                total_predictions += 1

    print("\n=== –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ú–û–î–ï–õ–ò ===")
    for cls in range(3):
        class_name = ["SELL", "HOLD", "BUY"][cls]
        pred_count = predictions_count[cls]
        correct_count = correct_predictions[cls]
        pred_percentage = pred_count / total_predictions * 100 if total_predictions > 0 else 0

        if pred_count > 0:
            accuracy = correct_count / pred_count * 100
        else:
            accuracy = 0

        print(f"{class_name}: {pred_count} preds ({pred_percentage:.1f}%), Accuracy: {accuracy:.1f}%")

    return predictions_count


def plot_confusion_matrix(model, dataloader, device):
    model.eval()
    model.to(device)

    all_targets = []
    all_predictions = []

    with torch.no_grad():
        for data, targets in dataloader:
            data = data.to(device)
            targets = targets.to(device)

            outputs = model(data)
            predictions = outputs.argmax(dim=1)

            # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ CPU –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            all_targets.extend(targets.cpu().numpy())
            all_predictions.extend(predictions.cpu().numpy())

    cm = confusion_matrix(all_targets, all_predictions)
    classes = ["SELL", "HOLD", "BUY"]

    print("\n=== –ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö ===")
    print("Rows = True, Columns = Predicted")
    print("     SELL HOLD BUY")
    for i, class_name in enumerate(classes):
        row = f"{class_name}: "
        for j in range(3):
            row += f"{cm[i][j]:4d} "
        print(row)

    # –í—ã—á–∏—Å–ª—è–µ–º precision –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    print("\n=== –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===")
    for i, class_name in enumerate(classes):
        precision = cm[i][i] / np.sum(cm[:, i]) if np.sum(cm[:, i]) > 0 else 0
        recall = cm[i][i] / np.sum(cm[i, :]) if np.sum(cm[i, :]) > 0 else 0
        print(f"{class_name}: Precision={precision:.3f}, Recall={recall:.3f}")

    return cm


def analyze_price_changes(data_dict, target_timeframe='M5'):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ü–µ–Ω"""
    if target_timeframe not in data_dict:
        print(f"‚ùå –¢–∞–π–º—Ñ—Ä–µ–π–º {target_timeframe} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö")
        return

    df = data_dict[target_timeframe]

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if hasattr(df, 'values'):
        data = df.values
    else:
        data = df

    price_changes = []

    for i in range(1, len(data)):
        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ü–µ–Ω (–∏–Ω–¥–µ–∫—Å 3 –¥–ª—è close price –≤ –º–∞—Å—Å–∏–≤–µ)
        current_price = data[i - 1][3] if len(data[i - 1]) > 3 else data[i - 1][0]  # close price
        future_price = data[i][3] if len(data[i]) > 3 else data[i][0]  # next close price

        change = (future_price - current_price) / current_price
        price_changes.append(change)

    price_changes = np.array(price_changes)

    print("=== –ê–ù–ê–õ–ò–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô –¶–ï–ù ===")
    print(f"–¢–∞–π–º—Ñ—Ä–µ–π–º: {target_timeframe}")
    print(f"–û–±—Ä–∞–∑—Ü–æ–≤: {len(price_changes)}")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {np.mean(price_changes) * 100:.3f}%")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {np.std(price_changes) * 100:.3f}%")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç: {np.max(price_changes) * 100:.3f}%")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ: {np.min(price_changes) * 100:.3f}%")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
    thresholds = [0.00005, 0.00007, 0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005]  # 0.01% –¥–æ 0.5%

    print(f"\n{'–ü–æ—Ä–æ–≥':>8} {'BUY':>8} {'SELL':>8} {'HOLD':>8} {'B+S':>8}")
    print("-" * 50)

    for threshold in thresholds:
        buy_count = np.sum(price_changes > threshold)
        sell_count = np.sum(price_changes < -threshold)
        hold_count = len(price_changes) - buy_count - sell_count

        print(f"{threshold * 100:7.3f}% {buy_count:7d} {sell_count:7d} {hold_count:7d} {buy_count + sell_count:7d}")

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞
    print(f"\n=== –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–û–î–ë–û–† –ü–û–†–û–ì–ê ===")
    target_ratio = 0.3  # –¶–µ–ª–µ–≤–∞—è –¥–æ–ª—è —Å–∏–≥–Ω–∞–ª–æ–≤ (30% BUY + 30% SELL)

    best_threshold = 0.001  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1%
    best_balance = float('inf')

    for threshold in np.linspace(0.00001, 0.01, 50):  # –æ—Ç 0.001% –¥–æ 1%
        buy_count = np.sum(price_changes > threshold)
        sell_count = np.sum(price_changes < -threshold)
        signal_ratio = (buy_count + sell_count) / len(price_changes)

        # –ò—â–µ–º –ø–æ—Ä–æ–≥, –¥–∞—é—â–∏–π –±–ª–∏–∑–∫–æ–µ –∫ —Ü–µ–ª–µ–≤–æ–º—É —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
        balance = abs(signal_ratio - target_ratio * 2) * 2
        # —Å–∏–≥–Ω–∞–ª–∞(BUY + SELL)

        if balance < best_balance:
            best_balance = balance
            best_threshold = threshold

    buy_count = np.sum(price_changes > best_threshold)
    sell_count = np.sum(price_changes < -best_threshold)
    hold_count = len(price_changes) - buy_count - sell_count

    print(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ—Ä–æ–≥: {best_threshold * 100:.3f}%")
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏ —ç—Ç–æ–º –ø–æ—Ä–æ–≥–µ:")
    print(f"  BUY: {buy_count} ({buy_count / len(price_changes) * 100:.1f}%)")
    print(f"  SELL: {sell_count} ({sell_count / len(price_changes) * 100:.1f}%)")
    print(f"  HOLD: {hold_count} ({hold_count / len(price_changes) * 100:.1f}%)")

    return best_threshold

# =============================================================================
# 5. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
# =============================================================================

def train_eurusd_model():
    """–§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è EURUSD"""

    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å–ø–∞–π–∫–æ–≤–æ–π ResNeXt –¥–ª—è EURUSD")

    # –ü–†–û–í–ï–†–ö–ê –ò –í–´–ë–û–† –£–°–¢–†–û–ô–°–¢–í–ê
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    if torch.cuda.is_available():
        print(f"üéØ GPU: {torch.cuda.get_device_name()}")
        print(f"üéØ CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
        print(f"üéØ –î–æ—Å—Ç—É–ø–Ω–æ GPU –ø–∞–º—è—Ç–∏: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")
    else:
        print("‚ö†Ô∏è  CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU (–æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω—ã–º)")

    mt5_available = initialize_mt5()

    if mt5_available:
        try:
            data_dict = download_mt5_data(symbol="EURUSDrfd", bars_count=20000)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ MT5: {e}")
            data_dict = create_mock_eurusd_data()
    else:
        data_dict = create_mock_eurusd_data()

    try:
        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        full_dataset = EURUSDDataset(
            data_dict=data_dict,
            lookback_window=80,
            num_steps=50,
            target_timeframe='M5'
        )

        print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã: {full_dataset.available_timeframes}")
        print(f"   –¶–µ–ª–µ–≤–æ–π —Ç–∞–π–º—Ñ—Ä–µ–π–º: {full_dataset.target_timeframe}")
        print(f"   –í—Å–µ–≥–æ samples: {len(full_dataset)}")

        # –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê TRAIN/VALIDATION
        train_size = int(0.8 * len(full_dataset))  # 80% –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        val_size = len(full_dataset) - train_size  # 20% –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º random_split –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size],
            generator=torch.Generator().manual_seed(42)  # –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        )

        print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   Train samples: {len(train_dataset)} ({len(train_dataset) / len(full_dataset) * 100:.1f}%)")
        print(f"   Validation samples: {len(val_dataset)} ({len(val_dataset) / len(full_dataset) * 100:.1f}%)")

        # –°–û–ó–î–ê–ï–ú DATALOADER –î–õ–Ø TRAIN –ò VALIDATION
        train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)
        val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # shuffle=False –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏

        # # –ü–†–û–í–ï–†–ö–ê –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø TARGETS
        # print("\n=== –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï TARGETS ===")
        # print("Train dataset:")
        # analyze_dataset(train_dataloader)
        # print("\nValidation dataset:")
        # analyze_dataset(val_dataloader)
        #
        # # –ê–ù–ê–õ–ò–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô –¶–ï–ù
        # recommended_threshold = analyze_price_changes(data_dict, 'M5')
        # print(f"Recommended threshold: {recommended_threshold}")

        # –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò
        model = MultiTimeframeSpikingResNeXt(
            num_timeframes=len(full_dataset.available_timeframes),
            num_classes=3,
            num_steps=50,
            cardinality=16,
            beta=0.9
        )

        # –ó–ê–ì–†–£–ó–ö–ê –í–ï–°–û–í (–ï–°–õ–ò –ï–°–¢–¨)
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏...")
        model_path = "saved_models/epoch_7_train_33.92_val_34.91.pth"
        if os.path.exists(model_path):
            state_dict = torch.load(model_path, map_location=device)
            model.load_state_dict(state_dict)
            print("‚úÖ –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        else:
            print("‚ö†Ô∏è  –§–∞–π–ª –≤–µ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å–æ —Å–ª—É—á–∞–π–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏")

        # –ü–ï–†–ï–ù–ï–°–¢–ò –ú–û–î–ï–õ–¨ –ù–ê –£–°–¢–†–û–ô–°–¢–í–û
        model = model.to(device)
        print(f"üß† –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è {len(full_dataset.available_timeframes)} —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤ –∏ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ {device}")

        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)

        scheduler = ReduceLROnPlateau(
            optimizer,
            mode='max',
            patience=2,  # –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π - –∂–¥–µ—Ç —Ç–æ–ª—å–∫–æ 2 —ç–ø–æ—Ö–∏
            factor=0.3,  # –°–∏–ª—å–Ω–µ–µ —É–º–µ–Ω—å—à–∞–µ—Ç LR (–≤ 3 —Ä–∞–∑–∞)
            min_lr=1e-7  # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –º–∏–Ω–∏–º—É–º
        )

        best_accuracy = 34.91
        val_accuracy = 34.91  # –í—Å—Ç–∞–≤–∏—Ç—å –∑–¥–µ—Å—å –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —Ç–æ–π –º–æ–¥–µ–ª–∏, —Å –∫–æ—Ç–æ—Ä–æ–π —Å—Ç–∞—Ä—Ç—É–µ–º
        models_dir = "saved_models"
        os.makedirs(models_dir, exist_ok=True)

        print("\nüéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")

        for epoch in range(8, 15):
            # === –§–ê–ó–ê –û–ë–£–ß–ï–ù–ò–Ø ===
            model.train()
            train_total_loss = 0
            train_correct = 0
            train_total = 0

            for batch_idx, (data, targets) in enumerate(train_dataloader):
                data = data.to(device)
                targets = targets.to(device)

                optimizer.zero_grad()
                outputs = model(data)
                loss = criterion(outputs, targets)

                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)
                scheduler.step(val_accuracy)

                train_total_loss += loss.item()
                _, predicted = outputs.max(1)
                train_total += targets.size(0)
                train_correct += predicted.eq(targets).sum().item()

                current_lr = optimizer.param_groups[0]['lr']

                if batch_idx % 50 == 0:
                    current_accuracy = 100. * predicted.eq(targets).sum().item() / targets.size(0)
                    print(f'{datetime.now()}, Epoch: {epoch}, Train Batch: {batch_idx}, Loss: {loss.item():.4f}, '
                          f'Batch Accuracy: {current_accuracy:.2f}%, LR: {current_lr:.2e}')

            train_accuracy = 100. * train_correct / train_total
            train_avg_loss = train_total_loss / len(train_dataloader)

            # === –§–ê–ó–ê –í–ê–õ–ò–î–ê–¶–ò–ò ===
            model.eval()
            val_total_loss = 0
            val_correct = 0
            val_total = 0

            with torch.no_grad():
                for data, targets in val_dataloader:
                    data = data.to(device)
                    targets = targets.to(device)

                    outputs = model(data)
                    loss = criterion(outputs, targets)

                    val_total_loss += loss.item()
                    _, predicted = outputs.max(1)
                    val_total += targets.size(0)
                    val_correct += predicted.eq(targets).sum().item()

            val_accuracy = 100. * val_correct / val_total
            val_avg_loss = val_total_loss / len(val_dataloader)

            # === –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
            print(f'\nüìä Epoch: {epoch}')
            print(f'   Train - Loss: {train_avg_loss:.4f}, Accuracy: {train_accuracy:.2f}%')
            print(f'   Val   - Loss: {val_avg_loss:.4f}, Accuracy: {val_accuracy:.2f}%')
            print(f'   Overfitting: {train_accuracy - val_accuracy:+.2f}%')

            # === –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ===
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
            torch.save(model.state_dict(),
                       f'{models_dir}/epoch_{epoch}_train_{train_accuracy:.2f}_val_{val_accuracy:.2f}.pth')
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: epoch_{epoch}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π accuracy
            if val_accuracy > best_accuracy:
                best_accuracy = val_accuracy
                torch.save(model.state_dict(), f'{models_dir}/best_model_val_acc_{val_accuracy:.2f}.pth')
                print(f"üèÜ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: val_acc={val_accuracy:.2f}%")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å
            torch.save(model.state_dict(), f'{models_dir}/latest_model.pth')

            # # === –ê–ù–ê–õ–ò–ó –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ù–ê VALIDATION ===
            # if epoch % 2 == 0:  # –ö–∞–∂–¥—ã–µ 2 —ç–ø–æ—Ö–∏
            #     print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—ç–ø–æ—Ö–∞ {epoch}):")
            #     analyze_model_predictions(model, val_dataloader, device)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        torch.save(model.state_dict(), f'{models_dir}/final_model_val_acc_{val_accuracy:.2f}.pth')
        print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: val_acc={val_accuracy:.2f}%")

        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

        # –§–ò–ù–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó
        print("\n=== –§–ò–ù–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó ===")
        # print("Train dataset predictions:")
        # analyze_model_predictions(model, train_dataloader, device)
        print("\nValidation dataset predictions:")
        analyze_model_predictions(model, val_dataloader, device)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

# =============================================================================
# 6. –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ú–´
# =============================================================================

if __name__ == "__main__":
    train_eurusd_model()